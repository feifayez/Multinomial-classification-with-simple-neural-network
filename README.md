# Multinomial-classification-with-simple-neural-network
With the help of various open-source libraries, such as TensorFlow, Keras, and Theano, etc. researchers can build neural network even with just a few lines of codes. However, how to build a trainable neural network that delivers highly accurate results reliably and consistently requires through understanding of the internal structure of the neural network. This study focused on the hidden layer and zoomed in on two important parameters: the number of nodes in the hidden layer and the activation function of hidden layer. A benchmark experiment in 2 x 2 factorial design utilizing the MNIST dataset was conducted to explore how these two parameters impact the accuracy of single-hidden-layer neural network using the backpropagation learning method. The results show that in general as the number of hidden nodes increase, the prediction accuracy of neural network increases. For the MNIST dataset, activation function relu outperforms sigmoid and tanh. To understand how hidden nodes influence output nodes, we examined the after-training weights of output layer of a chosen model. While some hidden nodes remain active for pretty much all output nodes (categories), some only influences certain output node (category). A simplified comparison analysis was also conducted to understand what hidden node corresponds to for each different class of input data. In general, classes that share similar shape of line at certain location of the image tend to trigger the hidden node to correspond to the same input nodes.
